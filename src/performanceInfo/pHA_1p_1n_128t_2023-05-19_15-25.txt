Command:        /mnt/irisgpfs/users/bvogel/src/pHA cuPP.txt 128 10 10 10 10 50 50 50 0 0 10 -info a
Resources:      1 node (128 physical, 128 logical cores per node)
Memory:         251 GiB per node
Tasks:          1 process, OMP_NUM_THREADS was 128
Machine:        aion-0312
Start time:     Fri May 19 2023 15:25:41 (UTC+02)
Total time:     923 seconds (about 15 minutes)
Full path:      /mnt/irisgpfs/users/bvogel/src

Summary: pHA is Compute-bound in this configuration
Compute:                                     99.9% |=========|
MPI:                                          0.0% |
I/O:                                          0.1% ||
This application run was Compute-bound. A breakdown of this time and advice for investigating further is in the CPU section below. 
As very little time is spent in MPI calls, this code may also benefit from running at larger scales.

CPU:
A breakdown of the 99.9% CPU time:
Single-core code:                             0.1% ||
OpenMP regions:                              99.9% |=========|
Scalar numeric ops:                           9.2% ||
Vector numeric ops:                           0.0% |
Memory accesses:                             55.9% |=====|
The per-core performance is memory-bound. Use a profiler to identify time-consuming loops and check their cache performance.
No time is spent in vectorized instructions. Check the compiler's vectorization advice to see why key loops could not be vectorized.

MPI:
A breakdown of the 0.0% MPI time:
Time in collective calls:                     0.0% |
Time in point-to-point calls:                 0.0% |
Effective process collective rate:            0.00 bytes/s
Effective process point-to-point rate:        0.00 bytes/s
No time is spent in MPI operations. There's nothing to optimize here!

I/O:
A breakdown of the 0.1% I/O time:
Time in reads:                                0.0% |
Time in writes:                             100.0% |=========|
Effective process read rate:                  0.00 bytes/s
Effective process write rate:                  518 kB/s
Most of the time is spent in write operations with a very low effective transfer rate. This may be caused by contention for the filesystem or inefficient access patterns. Use an I/O profiler to investigate which write calls are affected.

OpenMP:
A breakdown of the 99.9% time in OpenMP regions:
Computation:                                 58.9% |=====|
Synchronization:                             41.1% |===|
Physical core utilization:                  100.0% |=========|
System load:                                100.1% |=========|
Significant time is spent synchronizing threads in parallel regions. Check the affected regions with a profiler.
This may be a sign of overly fine-grained parallelism (OpenMP regions in tight loops) or workload imbalance.

Memory:
Per-process memory usage may also affect scaling:
Mean process memory usage:                    1.97 GiB
Peak process memory usage:                    3.36 GiB
Peak node memory usage:                      13.0% ||
The peak node memory usage is very low. Larger problem sets can be run before scaling to multiple nodes.

Energy:
A breakdown of how the 110 Wh was used:
CPU:                                        100.0% |=========|
System:                                   not supported
Mean node power:                          not supported
Peak node power:                              0.00 W
The whole system energy has been calculated using the CPU energy usage.
System power metrics: No Arm IPMI Energy Agent config file found in /var/spool/ipmi-energy-agent. Did you start the Arm IPMI Energy Agent?

